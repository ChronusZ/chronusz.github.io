---
layout: default
title: "Ripple Consensus Model 2"
date: 2017-06-19
use_math: true
---

# Convergence of Consensus

## Abstract

We discuss a model of Ripple consensus that aims to closely mirror the actual protocol in a mathematical framework. At the same time we develop a "model of reality" which aims to be a statistical model for how we expect things like network communication to behave during a round of consensus. We leave most parameters abstracted away as variables in the hope that analysis of the model will allow us to more wisely choose these parameters. We also toss around some ideas for how the model could be adapted to treat faults, but at the present we opt to put the fault-tolerant model on hold until we have a better understanding of the non-fault-tolerant model.

After developing the model we outline a plan for how we will analyze the model. We aim to derive formulas that can compute "tight" conditions on the safety of consensus, i.e., of the form: given a graph $$G$$ and a time $$T$$, what is the probability that on any given input the graph will come to consensus by time $$T$$?

## Model

I would like to use a model of consensus which behaves as a randomized model for the actual way that Ripple consensus works. I abstract away some of the parameters in the hope that we might be able to analyze which parameters give the best results, but I hope that plugging in the parameters currently in-use would make the model behave the same as the actual consensus algorithm under only the assumption that nodes behave consistently (i.e., according to some random distribution which does not vary over time) within each round of consensus. Since the time-span of a round of consensus is so short, I think this is a reasonable assumption to make and I believe shouldn't affect the probabilities too much. This does not account for the possibility of (Byzantine or crash) faults, but in the Treating Faults section below I discuss a few vague ideas about how to potentially include these faults in the analysis. If anyone thinks nodes behaving non-faultily shouldn't be modeled by a random process in the way I propose below, please let me hear your thoughts so I can possibly adjust the model if need be.

Also if anyone notices something about the model that does not match up with how Ripple consensus works, please let me know so that I can fix the model. I find the actual algorithm very confusing so there is a high chance that I messed something up. The only thing that I intentionally changed about the protocol was to make it so that people are voting on individual transactions rather than full ledgers. This is only a simplifying assumption to make the model more feasible. If this strikes anyone as something that could significantly affect the probabilities, again please let me hear your thoughts.

On to the model outline.

As always, let $$G=(V,E)$$ be a directed graph with adjacency matrix $$A$$. For a given node $$v\in V$$, let $$N^v$$ be the set of **neighbors** of $$v$$, i.e., $$N^v=\{u\in V\vert (u\to v)\in E\}$$. We use $$d(v)=\vert N^v\vert$$ to denote the **in-degree** of $$v$$. For simplicity we assume that each node in $$V$$ is indexed by a natural number, and we refer to the node and its index interchangeably. We assume that every node is its own neighbor. Let $$D$$ be the diagonal in-degree matrix. Define the **Laplacian operator** $$L$$ by $$L=D-A$$. We call $$X=\{0,1\}^V$$ the **state space** and elements $$x\in X$$ **state vectors**. The components of the state vector $$x$$ are denoted $$x^v$$ for $$v\in V$$. A matrix acts on a state vector $$x$$ by treating $$x$$ as an element of the vector space $$\mathbb{R}^V$$. The result of applying $$D^{-1}L$$ to the state vector $$x$$ is a (real) vector whose $$v$$-th component is the difference between $$x^v$$ and the average value across the neighbors of $$v$$. Although graph theorists like to call elements of $$V$$ vectors, we try to reserve this terminology for referring only to elements of $$X$$ or $$\mathbb{R}^V$$ and instead always refer to elements of $$V$$ as **nodes**.

We ignore relativistic issues and assume the existence of a continuous **absolute time** which is represented by a real number $$t\in[0,\infty)]$$. Each node keeps track of a discrete **internal timestep**, which is represented by a natural number $$n^v\in\mathbb{N}$$. We will often identify $$n^v$$ with the absolute time at which $$n^v$$ is called. Let $$\tau:\mathbb{N}\to[0.5,1)]$$ be a monotone-increasing function. We call $$\tau$$ the **threshold function**. For a given timestep $$n^v$$, $$\tau(n)$$ gives the current threshold of agreement for the node $$v$$; note that although $$\tau$$ takes as input the internal timestep of a node, the function is assumed to be the same for every node. Thus for instance, node $$a$$ may get to its third timestep after $$3.1$$ seconds while node $$b$$ might get to its third timestep after only $$2.8$$ seconds, but the threshold at that step will be the same for both $$a$$ and $$b$$.

We discuss in a later section some ideas for a "model of reality". The model of reality parameterizes everything that the nodes don't have total control over, like how quickly messages will arrive. The take-away is that we are given a collection of symbols $$dt^v$$ and $$dt^e$$, describing the speed of each node $$v\in V$$ and edge $$e\in E$$, respectively. The $$dt^v$$ are randomly chosen at the start, but stay constant throughout consensus. The $$dt^e$$ are randomly-chosen random distributions; in other words, at the start we randomly choose some distributions $$dt^e$$ of a certain type, and then whenever we send a message we can use the $$dt^e$$ to pick a random amount of time the transmission will take. The reason why $$dt^v$$ are constants while $$dt^e$$ are distributions is purely heuristic: it seems to me that a nonfaulty node would be highly unlikely to change computation speed significantly during a single round of consensus, whereas the speed of network communication can be highly variable.

Making all the $$dt^e$$ concrete variables would likely give sharper results, but allowing randomness allows us to account for "fringe behavior", like node $$A$$ broadcasting its state to nodes $$B$$ and $$C$$, $$B$$ updating its state according to the message from $$A$$ and broadcasting to $$C$$, and $$C$$ then receiving the message from $$B$$ before the message from $$A$$. This is of course unlikely, but could occur in practice so we should probably account for that probability to ensure that the network will behave at least as safely as we expect it to.

For a given absolute time $$t$$, let $$x_t$$ denote the current **absolute state vector** or just the **absolute state**. The absolute state is updated synchronously whenever a node goes through its timestep protocol. Note however that the absolute state is a purely formal construction; because of asynchrony, a node will in general not be aware of the current values of the absolute state, even for its immediate neighbors. For a given node $$v$$, we define the **local state** $$l_n^v\in\{0,1\}^{N^v}$$ to be the most recently received values of the neighbors of $$v$$ at timestep $$n^v$$. The components of $$l_n^v$$ are denoted $$l_n^{v,u}$$ for $$u\in N^v$$. The state is said to be **in consensus** when the absolute state vector is constant on every connected component.

The algorithm behaves for a given node $$v$$ at absolute time $$t$$ and internal time $$n^v$$ as follows:

1. For every neighbor $$u\in N^v$$, check the most recent message which has arrived from $$u$$, and update $$l_n^v$$ accordingly.
2. Compute $$S=\frac{1}{d(v)}\sum_{u\in N^v} l_n^{v,u}$$. If $$S>\tau(t^v)$$, set $$x^v=1$$. If $$S\leqslant\tau(n^v)$$, set $$x^v=0$$.
3. For each edge $$e\in E$$ pointing out from $$v$$, pick a random variable $$s$$ according to the distribution $$dt^e$$, and broadcast the new value of $$x^v$$ so that it arrives at time $$t+s$$.
4. Wait until time $$t+dt^v$$.

We assume the algorithm is allowed to run forever. We say the graph **comes to consensus** on an input vector $$x$$ if with probability $$1$$ the state is eventually in consensus for all times $$t$$. Obviously the algorithm does not actually run forever in practice; we want to be able to bound the runtime of the algorithm by some number $$T$$ and be effectively certain that the graph will come to consensus by time $$T$$ for any given input. Instead of starting out with a pre-defined bound however, we propose to start out assuming that the algorithm runs forever and later derive bounds on the probability that a given graph will come to consensus by time $$T$$. Then with these bounds

## Basic plan of attack

Here I outline the basic idea I have for analyzing the model.

We start by defining the **conflict** $$\eta(x)$$ of a state vector $$x\in X$$ as
\begin{aligned}
\eta(x):=\vert\vert Lx\vert\vert_1=\sum_{v\in V} \vert L^v\cdot x\vert,
\end{aligned}
where $$L^v$$ is the $$v$$-th row vector of the Laplacian matrix, and $$\cdot$$ is the usual dot product.

The conflict quantifies the total amount that the values of nodes differ from the values of their neighbors. It is easily seen to have the property that $$\eta(x)=0$$ iff $$x$$ is constant on each connected component. Up to rescaling of rows, the Laplacian matrix is the unique matrix for which $$\vert\vert Lx\vert\vert_1$$ has this property.

**Proposition 1**. For any fixed $$d,\varepsilon>0$$, suppose that for every time $$t$$, either $$\eta(x_t)=0$$ or the probability $$P(\eta(x_{t+d})<\eta(x_{t}))$$ is at least $$\varepsilon$$. Then the conflict will eventually drop to zero with probability $$1$$, so each connected component is guaranteed to achieve consensus eventually.

*Proof*: The proof is essentially just an appeal to "the infinite monkey theorem": if something can always happen with a time-invariant minimial probability which is nonzero, then it will eventually happen with probability $$1$$, regardless of how unlikely the event is.

Since there are only a finite number of of possible absolute states, there are only a finite number of possible values of $$\eta$$. Hence there exists an upper bound $$\Omega$$ on the values of $$\eta$$ and a lower bound $$\delta$$ on the distances between distinct values of $$\eta$$. Thus if $$\eta(x_{t+d})<\eta(x_{t})$$, then in fact $$\eta(x_{t+d})<\eta(x_{t})-\delta$$. Suppose starting at time $$t$$ if we repeatedly get lucky and decrease the conflict during every interval $$[t,t+d]$$. Then after $$k=\lceil\frac{\Omega}{\delta}\rceil$$ steps, if $$\eta(x_{t+kd})$$ were still positive we would have
\begin{aligned}
\eta(x_{t+kd})<\eta(x_t)-k\delta=\eta(x_t)-\left\lceil\frac{\Omega}{\delta}\right\rceil\delta\mathop{\leqslant}\limits^{!}\eta(x_t)-\left\lceil\frac{\eta(x_t)}{\delta}\right\rceil\delta<0,
\end{aligned}
where $$(!)$$ uses the fact that $$\Omega$$ is an upper bound for $$\eta$$. But this contradicts positive-definiteness of $$\eta$$, so $$\eta(x_{t+kd})=0$$. But the probability of getting a lucky round at any time $$t$$ is bounded below by $$\varepsilon>0$$ by hypothesis, so at each time $$t$$ there is a probability of at least $$\varepsilon^k>0$$ that the conflict will be zero at time $$t+kd$$. Since the quantity $$\varepsilon^k$$ is positive and independent of $$t$$, by the infinite monkey theorem eventually the conflict will drop to zero with probability $$1$$.⬜

A graph satisfying the hypothesis of proposition $$1$$ is called **locally controlled**. Thus we can convert the problem of determining when a graph will come to consensus into the problem of determining whether it is locally controlled. This problem seems a lot easier to me; it is similar in spirit to the way that analysts working on nonlinear PDEs will often try to find some "controlled quantity" like "energy" or "entropy" that behaves in a controlled way under the evolution of the PDE, and use the behavior of the controlled quantity to study the behavior of the PDE.

I think that being locally controlled is actually strictly stronger than the condition that each connected component eventually achieves consensus. One could imagine a situation in which one is stuck in a state of local minimum conflict and has to increase their conflict temporarily, but eventually they're still guaranteed to eventually drop down to zero after getting "unstuck". However, local control is still a far weaker property than what we want in practice; even if one is guaranteed to eventually reach consensus, if $$k$$ is large and $$\varepsilon$$ is small, a probability of $$\varepsilon^k$$ can be tiny! It doesn't matter if our graph is guaranteed to converge if we can only guarantee convergence after ten billion years. Hence I don't think anything is lost by restricting to locally controlled graphs, since adding the other graphs which converge but aren't locally controlled will probably only add graphs which converge even slower.

My first aim is to classify the graphs which are locally controlled. This should give a "baseline" class of graphs; certainly every graph we consider should be locally controlled.

Although locally controlled graphs form a baseline class, we would like a class of graphs which converge faster. I think this can be done in three steps:

1. Derive a formula for the EXPECTED change in entropy as a function of the current time. This will likely depend heavily on the parameters used to define the evolution of the model, like $$\tau$$ and $$dt$$. It may be too intractible to derive the exact expectation in the general case, so I might make certain simplifying assumptions on the model or compute the expectation with some tightly bounded error. A locally controlled graph which has always has a non-positive expected change in entropy is called **locally decreasing**. A graph which has a negative expected change in entropy for any non-con is called **quickly decreasing**. Locally decreasing graphs
2. Using the expectations from the previous step, derive a formula for the probability that consensus will have converged by absolute time $$T$$. Again, it might be necessary to simplify the model or allow some tightly bounded error. If nothing else we could hope to find a formula for a lower bound on the probability and hope that it's reasonably tight, possibly refining it later if it turns out to be too weak.
3. Using the probabilities from the previous step, classify graphs for which the probability of consensus having converged by absolute time $$T$$ is at least $$1-\varepsilon$$. If the full classification is too hard, we can at least try to check the property for certain heuristically likely classes of random graphs, like the directed scale-free graphs of Bollobas et al.

Even without step $$3$$, the "tight" results given by step $$2$$ should be very useful for making heuristic judgements about the safety of consensus; without tight bounds on the probability that consensus converges, one needs to restrict to the trivial case where everything is guaranteed to converge to a state which is immediately verifiable. But even if one wants to require the probability of convergence within time $$T$$ to be as high as $$99.9999999\%$$ or something, I predict there will be a lot more such graphs than just the trivial ones. Note however that the probabilities will likely depend a lot on the distributions of $$dt$$. For this reason it will be quite important to have data about what we can reasonably expect these distributions to be. I don't know how we can get this data though.

Further, since we have abstracted away the function $$\tau$$, we can use the results from step $$2$$ to attempt to reverse-engineer the function which optimizes convergence. One has to be careful when doing this though I imagine, since for example setting $$\tau$$ to be the constant function with value $$0.999$$ will for reasonably sized graphs always ensure immediate convergence, but clearly it does not give the behavior we want. Thus if we want to solve the variantional problem to find the best $$\tau$$, we'll have to put some constraints on it, and I'm not sure what these constraints could reasonably be. Maybe something relating the proportion of nodes which start out saying $$1$$ to the probability that the graph converges to all $$1$$'s would work. Also I don't really expect $$\tau$$ can be improved significantly; I imagine for any "reasonable" choice of $$\tau$$ (i.e., one which doesn't generally reject reasonably popular transactions) the effect of $$\tau$$ on the probability computations will be minor. But it will still be good nonetheless to have mathematical evidence that this is the case.

## Model of Reality

If all we wanted to do was make statements about absolute safety, then we would be forced to consider only the graphs which can be immediately validated in a single step with absolute certaint; this leads to stringent requirements on the tightness of the network like the condition described in the Ripple consensus whitepaper. The problem with this condition on absolute safety is that one might reasonably expect that there are a wide class of graphs which can't always be immediately validated, but which can usually be immediately validated and the states that aren't immediately validated are almost always quickly validated, and further these states are highly unstable under the running of consensus and thus are highly unlikely to be output by consensus. Thus we can be highly confident that a given graph will always output the correct answer, even though under some storm of bad luck it might fail. Here we run into an issue with our informal language though: we want Ripple to run for a very long time and never fail, so even if a graph fails at most only once in a million times, that's too unstable. But if the graph only failed at most once in a billion times, then it's probably safe. But informal language will never be able to quantify the statement "the graph fails at most once in X times", so from the point of view of informal language high confidence is never enough.

Thus we want a model of reality: a model which randomly gives timing values to the system so that we can quantify probabilistically over all possible running environments and get an exact and certain lower bound on the safety of a given graph. Here though we need to be careful to choose an accurate model: if we choose a model of absolute synchrony, clearly it won't behave in the same way as Ripple consensus does in practice. More subtly, however, simply things like slightly decreasing the standard variation on how quickly messages are broadcast along an edge might have a serious impact on the chance of a graph converging quickly. Optimally I would really like to have explicit operational data on things like how long it takes messages to be received or how much computer speed can actually affect the length of local timesteps for a given node. Without this data the only two options are to 1) pick an arbitrary model based on intuition about what seems likely, or 2) make the model as general as possible and abstract everything away so that once we actually get the data we can set the parameters and make the computations.

As for 1) I expect it would be something like this: the speeds of edge transmission follow a Gamma distribution with a random mean. The distribution of the random means depends on things like the in-degree of the input node (nodes with higher in-degrees are being relied on more for consensus, so it's more imperitive that they communicate with others effectively). The timesteps of the nodes are constant but vary node-to-node. The minimal timestep is absolute time $$1$$ and the maximal timestep is unbounded above. The distribution of timesteps is also a Gamma distribution with mean dependent on in-degree, so that a node is more likely to have a timestep near absolute time $$1$$ if it has a higher in-degree.

This model already makes a number of assumptions, and I haven't even gone into explicitly parameterizing the distributions. I feel uncomfortable getting any more specific than that. Again, the issue with 1) is that we want quantified estimates on the likelihood of consensus succeeding, so adding in arbitrary paramaters which may be false in reality completely undermines the task.

Choice 2) is perhaps better, but it has its own share of problems. Again, we want to be able to make quantified predictions with the model; if the formula for making those predictions is filled with parameters, it could make the formula completely unwieldly. If all we wanted to do is compute the probability that a specific graph will converge quickly, then this might not be such a problem: just play around with the parameters, try out all the parameters that seem reasonable, and if the graph is safe under every parameterization you can think of, then the graph itself is probably safe. But we're also interesting in *classifying* the graphs which come to consensus quickly, and for this we need a specific model since different models will give different classifications. And even for a single graph where we try out various parameterizations, we're still just making arbitrary heuristic choices in the end, so all we've done is push back the problem to a later date and in doing so probably made deriving the formula itself a lot harder.

For now I choose to specify the bare minimum of parameters that seem reasonable to me, and leave everything abstracted away for later. If anyone wants to discuss this parameterization, I would be happy to talk. I'm not particularly attached to this model, so if anyone can convince of a more reasonable model I'll switch. Note as well that I know a fair bit of probability theory, but not very much statistics, so a model that sounds reasonable to me may not be at all reasonable.

Let $$\mu:\mathbb{N}\to (0,\infty)$$ be a monotone decreasing function, thought of as the average speed of a node with a given in-degree. Let $$\theta\in (0,\infty)$$ be a number, thought of as a sort of general "randomness" of parameters. Let $$\rho\in(0,\infty)$$ be a number, thought of as how much the timesteps of the nodes can deviate from the absolute timestep. For each $$e=(v\to u)\in E$$, let $$d(e)=\vert N^v\vert$$, i.e., the in-degree of the input to $$e$$. For each $$v\in V$$, let $$\sigma^v:(0,\infty)\to(0,\infty)$$ be a random variable with distribution $$\Gamma(\frac{\mu(d(v))}{\theta},\theta)$$, where $$\Gamma(k,\theta)$$ is the Gamma distribution with shape parameter $$k$$ and scale parameter $$\theta$$. Similarly define random variables $$\sigma^e:(0,\infty)\to(0,\infty)$$ with distribution $$\Gamma(\frac{\mu(d(e))}{\theta},\theta)$$ for every $$e\in E$$. The variables $$\sigma$$ are thought of as the average speed of the given node or edge.

For every $$e\in E$$, define the random distribution $$dt^e=\Gamma(\frac{\sigma^e}{\theta},\theta)$$. The distribution $$dt^e$$ describes the likelihood that a given message broadcast along $$e$$ will take a given length of time to be received. Note that under this interpretation there is no hard lower bound to the speed at which a message can be sent; the probability of taking time $$t$$ to sent goes to zero as $$t\to 0$$ by the general properties of the Gamma distribution (and it does so exponentially fast for low values of $$\theta$$), but it could theoretically arrive arbitrarily quickly. I don't know how realistic this is in practice, but setting a hard lower bound seems even more unrealistic.

In contrast, the time step of a given node $$v\in V$$ is the constant value $$dt^v=1+\rho\sigma^v$$. Thus, although the speed of the nodes can vary randomly at the start, after the speed is chosen it stays constant; since we expect the length of consensus rounds to be on the order of a few seconds, this is probably a fairly reasonable assumption for the nonfaulty nodes, since it's pretty unlikely that the speed of one's computer will change significantly during the course of a single round. I would also expect $$\rho$$ to be pretty small in practice, so the differece things could make are probably minimal anyway. See however, the section on Treating Faults for a discussion of how faults come to play. Note also that in contrast to the edge transmission speeds, there is a hard lower bound on the timestep of the nodes. This is a part of the actual Ripple consensus algorithm, so it's probably reasonable to include. I'm not sure however whether it might be possible to have one's local clock running more quickly than the absolute clock, and whether this would allow one to update more quickly. If so I don't know whether or not it's worth removing the hard lower bound.

**tl;dr:** $$dt^e$$ is a randomly chosen random distribution giving the absolute-time length-of-transmission for each message sent along $$e$$. It can take any positive value, but it has a mean of $$\mu(v)$$ on average, where $$v$$ is the input node of $$e$$. $$dt^v$$ is a randomly chosen constant giving the absolute-time length-of-updates for each node. It is bounded below by $$1$$. $$\theta$$ is a general "randomness" parameter, and $$\rho$$ describes how much the timesteps of nodes varies above $$1$$.

## Treating Faults

TBD
